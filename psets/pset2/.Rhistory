# put Nonconp and Treated in the first columns
dollars_data <- dollars_data %>%
select(Treated, TotAds, NonComp, everything())
View(dollars_data)
head(dollars_data)
# How many treated vs not treated
dollars_data %>%
group_by(Treated) %>%
summarise(n = n())
library(readstata13)
# Load the dta data
dollars_data <- read.dta13("dollars_on_the_sidewalk.dta")
# define binary Treated variable for TotAds > 1000 and in Noncomp = 1 states
dollars_data$Treated <- ifelse(dollars_data$TotAds > 1000 & dollars_data$NonComp == 1, 1, 0)
# put Nonconp and Treated in the first columns
dollars_data <- dollars_data %>%
select(Treated, TotAds, NonComp, everything())
head(dollars_data)
# How many treated vs not treated
dollars_data %>%
group_by(Treated) %>%
summarise(n = n())
# Estimate the mean contribution for each level of the Treated variable
dollars_data %>%
group_by(Treated) %>%
summarise(mean_contribution = mean(Contribution, na.rm = TRUE))
# Estimate the mean contribution for each level of the Treated variable
dollars_data %>%
group_by(Treated) %>%
summarise(mean_contribution = mean(Cont, na.rm = TRUE))
# Estimate Naive ATE
lm_naive <- lm(Cont ~ Treated, data = dollars_data)
naive_ate <- coef(lm_naive)[2]
# Estimate the mean contribution for each level of the Treated variable
dollars_data %>%
group_by(Treated) %>%
summarise(mean_contribution = mean(Cont, na.rm = TRUE))
# Estimate Naive ATE
lm_naive <- lm(Cont ~ Treated, data = dollars_data)
naive_ate <- coef(lm_naive)[2]
cat("Naive ATE: ", naive_ate, "\n")
# Estimate the mean contribution for each level of the Treated variable
dollars_data %>%
group_by(Treated) %>%
summarise(mean_contribution = mean(Cont, na.rm = TRUE))
# Estimate Naive ATE
lm_naive <- lm(Cont ~ Treated, data = dollars_data)
naive_ate <- coef(lm_naive)[2]
cat("Naive ATE: ", naive_ate, "\n")
colnames(dollars_data)
# Step 1: Full Logistic Regression Model with All Covariates
full_model <- glm(Treated ~ ., data = dollars_data, family = binomial(link = "logit"))
library(MASS)
library(MatchIt)
library(ggplot2)
# Step 1: Full Logistic Regression Model with All Covariates
full_model <- glm(Treated ~ ., data = dollars_data, family = binomial(link = "logit"))
# Check for covariates with only one unique value
single_level_vars <- sapply(df, function(x) length(unique(x)))
# Check for covariates with only one unique value
single_level_vars <- sapply(dollars_data, function(x) length(unique(x)))
single_level_vars[single_level_vars <= 1]
# Remove single-level variables
df_filtered <- dollars_data[, sapply(dollars_data, function(x) length(unique(x)) > 1)]
# Verify remaining columns
colnames(df_filtered)
# Check for covariates with only one unique value
single_level_vars <- sapply(df_filtered, function(x) length(unique(x)))
single_level_vars[single_level_vars <= 1]
# Step 1: Full Logistic Regression Model with All Covariates
full_model <- glm(Treated ~ ., data = df_filtered, family = binomial(link = "logit"))
# Remove constant and ID-like variables
covariates_to_exclude <- c("zip", "StCtyFIPS", "DMAName", "State", "_merge1", "_merge2", "_merge3", "_merge4")
# Filter dataset to keep useful covariates
df_filtered <- df[ , !(names(dollars_data) %in% covariates_to_exclude)]
# Remove constant and ID-like variables
covariates_to_exclude <- c("zip", "StCtyFIPS", "DMAName", "State", "_merge1", "_merge2", "_merge3", "_merge4")
# Filter dataset to keep useful covariates
df_filtered <- dollars_data[ , !(names(dollars_data) %in% covariates_to_exclude)]
# Remove single-level variables
df_filtered <- df_filtered[, sapply(df_filtered, function(x) length(unique(x)) > 1)]
# Step 1: Full Logistic Regression Model with All Covariates
full_model <- glm(Treated ~ ., data = df_filtered, family = binomial(link = "logit"))
colnames(df_filtered)
# Step 1: Full Logistic Regression Model with All Covariates
full_model <- glm(Treated ~ ., data = df_filtered, family = binomial(link = "logit"))
# Convert character columns to factors
df_filtered <- df_filtered %>%
mutate(across(where(is.character), as.factor))
# Check factor levels to identify potential issues
sapply(df_filtered, function(x) if (is.factor(x)) levels(x))
library(readstata13)
# Load the dta data
dollars_data <- read.dta13("dollars_on_the_sidewalk.dta")
# define binary Treated variable for TotAds > 1000 and in Noncomp = 1 states
dollars_data$Treated <- ifelse(dollars_data$TotAds > 1000 & dollars_data$NonComp == 1, 1, 0)
# put Nonconp and Treated in the first columns
dollars_data <- dollars_data %>%
select(Treated, TotAds, NonComp, everything())
dollars_data
# put Nonconp and Treated in the first columns
dollars_data <- dollars_data %>%
select(Treated, TotAds, NonComp, everything())
library(readstata13)
library(dplyr)
# Load the dta data
dollars_data <- read.dta13("dollars_on_the_sidewalk.dta")
# define binary Treated variable for TotAds > 1000 and in Noncomp = 1 states
dollars_data$Treated <- ifelse(dollars_data$TotAds > 1000 & dollars_data$NonComp == 1, 1, 0)
# put Nonconp and Treated in the first columns
dollars_data <- dollars_data %>%
select(Treated, TotAds, NonComp, everything())
colnames(dollars_data)
# this chunk contains code that sets global options for the entire .Rmd.
# we use include=FALSE to suppress it from the top of the document, but it will still appear in the appendix.
knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE, linewidth=60)
# you can include your libraries here:
library(tidyverse)
# and any other options in R:
options(scipen=999)
set.seed(123)
n_obs <- 10000
tau <- 25000
# Create the dataset
U1 <- rbinom(n_obs, 1, 0.5)
U2 <- rbinom(n_obs, 1, 0.5)
X2 <- sample(1:50, n_obs, replace = TRUE)
X1 <- 10 + 1500*U1 + 1500*U2 + rnorm(n_obs, mean = 0, sd = 100)
Y0 <- 20000 + 1000*X2 + 1500*U2 + rnorm(n_obs, mean = 0, sd = 1000)
Y1 <- Y0 + tau
prob_d <- pnorm(X2, mean = 50, sd = 25) + 0.5*U1
prob_d <- pmin(prob_d, 1)
D <- rbinom(n_obs, 1, prob_d)
sim_data <- data.frame(U1, U2, X1, X2, Y0, Y1, D)
sim_data$Y <- ifelse(sim_data$D== 1, Y1, Y0)
head(sim_data)
library(dagitty)
dag <- dagitty('dag {
D -> Y
X1 -> D
X2 -> D
X1 -> Y
X2 -> Y
}')
plot(dag)
library(ggplot2)
sim_data %>%
ggplot(aes(x = Y0, fill = factor(D))) +
geom_density(alpha = 0.5) +
labs(title = "Density plot of Y0 split by treatment status",
x = "Y0",
y = "Density")
#install.packages("tableone")
library(tableone)
tableone <- CreateTableOne(vars = c("X1", "X2"), strata = "D", data = sim_data)
print(tableone, nonnormal = c("X1", "X2"))
lm_naive <- lm(Y ~ D, data = sim_data)
#summary(lm_naive)
naive_ate <- coef(lm_naive)[2]
true_ate <- mean(sim_data$Y1 - sim_data$Y0)
cat("True ATE: ", true_ate, "\n")
cat("Naive ATE: ", naive_ate, "\n")
bias <- naive_ate - true_ate
cat("Selection Bias: ", bias, "\n")
lm_x2 <- lm(Y ~ D + X2, data = sim_data)
x2_ate <- coef(lm_x2)[2]
cat("X2 ATE: ", x2_ate, "\n")
lm_x1 <- lm(Y ~ D + X1, data = sim_data)
x1_ate <- coef(lm_x1)[2]
cat("X1 ATE: ", x1_ate, "\n")
library(ggplot2)
# Define number of simulations
n_sim <- 1000
n_obs <- 10000
tau <- 25000
# Store biases for each model
bias_naive <- numeric(n_sim)
bias_x2 <- numeric(n_sim)
bias_x1 <- numeric(n_sim)
# Monte Carlo simulation loop
for (i in 1:n_sim) {
# Generate fresh data
U1 <- rbinom(n_obs, 1, 0.5)
U2 <- rbinom(n_obs, 1, 0.5)
X2 <- sample(1:50, n_obs, replace = TRUE)
X1 <- 10 + 1500 * U1 + 1500 * U2 + rnorm(n_obs, mean = 0, sd = 100)
Y0 <- 20000 + 1000 * X2 + 1500 * U2 + rnorm(n_obs, mean = 0, sd = 1000)
Y1 <- Y0 + tau
prob_d <- pnorm(X2, mean = 50, sd = 25) + 0.5 * U1
prob_d <- pmin(prob_d, 1)
D <- rbinom(n_obs, 1, prob_d)
Y <- ifelse(D == 1, Y1, Y0)
# Store data in a dataframe
sim_data <- data.frame(U1, U2, X1, X2, Y0, Y1, D, Y)
# Compute true ATE (from counterfactuals)
true_ate <- mean(sim_data$Y1 - sim_data$Y0)
# Estimate ATE under different models
naive_ate <- coef(lm(Y ~ D, data = sim_data))[2]
x2_ate <- coef(lm(Y ~ D + X2, data = sim_data))[2]   # Adding X2 to base model
x1_ate <- coef(lm(Y ~ D + X1, data = sim_data))[2]   # Adding X1 only (corrected for 2.6)
# Store biases
bias_naive[i] <- naive_ate - true_ate
bias_x2[i] <- x2_ate - true_ate
bias_x1[i] <- x1_ate - true_ate
}
# Create a data frame for visualization
bias_data <- data.frame(
Bias = c(bias_naive, bias_x2, bias_x1),
Model = rep(c("Naïve", "With X2", "With X1"), each = n_sim)
)
# Plot histograms of the biases
ggplot(bias_data, aes(x = Bias, fill = Model)) +
geom_histogram(alpha = 0.6, position = "identity", bins = 50) +
facet_wrap(~ Model, scales = "free") +
geom_vline(aes(xintercept = mean(Bias)), color = "black", linetype = "dashed") +
labs(
title = "Distribution of ATE Bias Across 1,000 Simulations",
x = "Bias (Estimated ATE - True ATE)",
y = "Frequency"
) +
theme_minimal()
library(readstata13)
# Load the dta data
dollars_data <- read.dta13("dollars_on_the_sidewalk.dta")
# define binary Treated variable for TotAds > 1000 and in Noncomp = 1 states
dollars_data$Treated <- ifelse(dollars_data$TotAds > 1000 & dollars_data$NonComp == 1, 1, 0)
colnames(dollars_data)
# put Nonconp and Treated in the first columns
dollars_data <- dollars_data %>%
select(Treated, TotAds, NonComp, everything())
head(dollars_data)
# How many treated vs not treated
dollars_data %>%
group_by(Treated) %>%
summarise(n = n())
# Estimate the mean contribution for each level of the Treated variable
dollars_data %>%
group_by(Treated) %>%
summarise(mean_contribution = mean(Cont, na.rm = TRUE))
library(MASS)
library(MatchIt)
library(ggplot2)
# Remove constant and ID-like variables
covariates_to_exclude <- c("zip", "StCtyFIPS", "DMAName", "State", "_merge1", "_merge2", "_merge3", "_merge4")
# Filter dataset to keep useful covariates
df_filtered <- dollars_data[ , !(names(dollars_data) %in% covariates_to_exclude)]
# Remove single-level variables
df_filtered <- df_filtered[, sapply(df_filtered, function(x) length(unique(x)) > 1)]
# Convert character columns to factors
df_filtered <- df_filtered %>%
mutate(across(where(is.character), as.factor))
# Check factor levels to identify potential issues
sapply(df_filtered, function(x) if (is.factor(x)) levels(x))
# Step 1: Full Logistic Regression Model with All Covariates
full_model <- glm(Treated ~ ., data = df_filtered, family = binomial(link = "logit"))
# Remove constant and ID-like variables
covariates_to_exclude <- c("zip", "StCtyFIPS", "DMAName", "State", "_merge1", "_merge2", "_merge3", "_merge4")
# Filter dataset to keep useful covariates
df_filtered <- dollars_data[ , !(names(dollars_data) %in% covariates_to_exclude)]
# Remove single-level variables
df_filtered <- df_filtered[, sapply(df_filtered, function(x) length(unique(x)) > 1)]
# Identify high-cardinality columns
high_cardinality_vars <- c("zip", "DMAName", "Market", "Zip_Number", "geoid", "geodisplaylabel")
# Remove them from the dataset
df_filtered <- df_filtered %>%
select(-all_of(high_cardinality_vars))
library(MASS)
library(MatchIt)
library(ggplot2)
# Remove variables
covariates_to_exclude <- c("zip", "StCtyFIPS", "DMAName", "Market", "Zip_Number", "geoid", "geodisplaylabel", "State", "_merge1", "_merge2", "_merge3", "_merge4")
# Filter dataset to keep useful covariates
df_filtered <- dollars_data[ , !(names(dollars_data) %in% covariates_to_exclude)]
# Remove single-level variables if any
df_filtered <- df_filtered[, sapply(df_filtered, function(x) length(unique(x)) > 1)]
# Step 1: Full Logistic Regression Model with All Covariates
full_model <- glm(Treated ~ ., data = df_filtered, family = binomial(link = "logit"))
# Identify high-cardinality categorical variables
high_cardinality_vars <- sapply(df_filtered, function(x) if (is.factor(x)) length(unique(x)) else NA)
high_cardinality_vars <- names(high_cardinality_vars[high_cardinality_vars > 50])
# Remove high-cardinality variables
df_filtered <- df_filtered[, !(names(df_filtered) %in% high_cardinality_vars)]
# Step 1: Full Logistic Regression Model with All Covariates
full_model <- glm(Treated ~ ., data = df_filtered, family = binomial(link = "logit"))
# Check missing values
colSums(is.na(df_filtered))
# Remove rows with missing values (or impute if appropriate)
df_filtered <- na.omit(df_filtered)
df_filtered <- df_filtered %>%
drop_na()
# Step 1: Full Logistic Regression Model with All Covariates
full_model <- glm(Treated ~ ., data = df_filtered, family = binomial(link = "logit"))
colnames(dollars_data)
colnames(dollars_data)
# Fit logiszic regression model to estimate propensity scores for each observation
glm_propensity <- glm(Treated ~ TotAds + NonComp + density + MedianHHInc + MaleOver65 + FemaleOver65 + Rural + Urban + PercentWhite + PercentBlack + PercentHispanic , data = dollars_data, family = "binomial"(link = "logit"))
# Step 1: Select relevant columns
selected_data <- dataset %>%
select(Treated, Cont, TotalPop, MedianHHInc, PercentOver65, PercentWhite, PercentBlack,
PercentHispanic, Rural, Urban, repvote00, repvote04, RepubVote, RepubShare,
per_hsgrads, per_collegegrads, density)
# Step 1: Select relevant columns
selected_data <- dollars_data %>%
select(Treated, Cont, TotalPop, MedianHHInc, PercentOver65, PercentWhite, PercentBlack,
PercentHispanic, Rural, Urban, repvote00, repvote04, RepubVote, RepubShare,
per_hsgrads, per_collegegrads, density)
library(readstata13)
# Load the dta data
dollars_data <- read.dta13("dollars_on_the_sidewalk.dta")
# define binary Treated variable for TotAds > 1000 and in Noncomp = 1 states
dollars_data$Treated <- ifelse(dollars_data$TotAds > 1000 & dollars_data$NonComp == 1, 1, 0)
colnames(dollars_data)
# put Nonconp and Treated in the first columns
dollars_data <- dollars_data %>%
select(Treated, TotAds, NonComp, everything())
# this chunk contains code that sets global options for the entire .Rmd.
# we use include=FALSE to suppress it from the top of the document, but it will still appear in the appendix.
knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE, linewidth=60)
# you can include your libraries here:
library(tidyverse)
# and any other options in R:
options(scipen=999)
set.seed(123)
n_obs <- 10000
tau <- 25000
# Create the dataset
U1 <- rbinom(n_obs, 1, 0.5)
U2 <- rbinom(n_obs, 1, 0.5)
X2 <- sample(1:50, n_obs, replace = TRUE)
X1 <- 10 + 1500*U1 + 1500*U2 + rnorm(n_obs, mean = 0, sd = 100)
Y0 <- 20000 + 1000*X2 + 1500*U2 + rnorm(n_obs, mean = 0, sd = 1000)
Y1 <- Y0 + tau
prob_d <- pnorm(X2, mean = 50, sd = 25) + 0.5*U1
prob_d <- pmin(prob_d, 1)
D <- rbinom(n_obs, 1, prob_d)
sim_data <- data.frame(U1, U2, X1, X2, Y0, Y1, D)
sim_data$Y <- ifelse(sim_data$D== 1, Y1, Y0)
head(sim_data)
library(dagitty)
dag <- dagitty('dag {
D -> Y
X1 -> D
X2 -> D
X1 -> Y
X2 -> Y
}')
plot(dag)
library(ggplot2)
sim_data %>%
ggplot(aes(x = Y0, fill = factor(D))) +
geom_density(alpha = 0.5) +
labs(title = "Density plot of Y0 split by treatment status",
x = "Y0",
y = "Density")
#install.packages("tableone")
library(tableone)
tableone <- CreateTableOne(vars = c("X1", "X2"), strata = "D", data = sim_data)
print(tableone, nonnormal = c("X1", "X2"))
lm_naive <- lm(Y ~ D, data = sim_data)
#summary(lm_naive)
naive_ate <- coef(lm_naive)[2]
true_ate <- mean(sim_data$Y1 - sim_data$Y0)
cat("True ATE: ", true_ate, "\n")
cat("Naive ATE: ", naive_ate, "\n")
bias <- naive_ate - true_ate
cat("Selection Bias: ", bias, "\n")
lm_x2 <- lm(Y ~ D + X2, data = sim_data)
x2_ate <- coef(lm_x2)[2]
cat("X2 ATE: ", x2_ate, "\n")
lm_x1 <- lm(Y ~ D + X1, data = sim_data)
x1_ate <- coef(lm_x1)[2]
cat("X1 ATE: ", x1_ate, "\n")
library(ggplot2)
# Define number of simulations
n_sim <- 1000
n_obs <- 10000
tau <- 25000
# Store biases for each model
bias_naive <- numeric(n_sim)
bias_x2 <- numeric(n_sim)
bias_x1 <- numeric(n_sim)
# Monte Carlo simulation loop
for (i in 1:n_sim) {
# Generate fresh data
U1 <- rbinom(n_obs, 1, 0.5)
U2 <- rbinom(n_obs, 1, 0.5)
X2 <- sample(1:50, n_obs, replace = TRUE)
X1 <- 10 + 1500 * U1 + 1500 * U2 + rnorm(n_obs, mean = 0, sd = 100)
Y0 <- 20000 + 1000 * X2 + 1500 * U2 + rnorm(n_obs, mean = 0, sd = 1000)
Y1 <- Y0 + tau
prob_d <- pnorm(X2, mean = 50, sd = 25) + 0.5 * U1
prob_d <- pmin(prob_d, 1)
D <- rbinom(n_obs, 1, prob_d)
Y <- ifelse(D == 1, Y1, Y0)
# Store data in a dataframe
sim_data <- data.frame(U1, U2, X1, X2, Y0, Y1, D, Y)
# Compute true ATE (from counterfactuals)
true_ate <- mean(sim_data$Y1 - sim_data$Y0)
# Estimate ATE under different models
naive_ate <- coef(lm(Y ~ D, data = sim_data))[2]
x2_ate <- coef(lm(Y ~ D + X2, data = sim_data))[2]   # Adding X2 to base model
x1_ate <- coef(lm(Y ~ D + X1, data = sim_data))[2]   # Adding X1 only (corrected for 2.6)
# Store biases
bias_naive[i] <- naive_ate - true_ate
bias_x2[i] <- x2_ate - true_ate
bias_x1[i] <- x1_ate - true_ate
}
# Create a data frame for visualization
bias_data <- data.frame(
Bias = c(bias_naive, bias_x2, bias_x1),
Model = rep(c("Naïve", "With X2", "With X1"), each = n_sim)
)
# Plot histograms of the biases
ggplot(bias_data, aes(x = Bias, fill = Model)) +
geom_histogram(alpha = 0.6, position = "identity", bins = 50) +
facet_wrap(~ Model, scales = "free") +
geom_vline(aes(xintercept = mean(Bias)), color = "black", linetype = "dashed") +
labs(
title = "Distribution of ATE Bias Across 1,000 Simulations",
x = "Bias (Estimated ATE - True ATE)",
y = "Frequency"
) +
theme_minimal()
library(readstata13)
# Load the dta data
dollars_data <- read.dta13("dollars_on_the_sidewalk.dta")
# define binary Treated variable for TotAds > 1000 and in Noncomp = 1 states
dollars_data$Treated <- ifelse(dollars_data$TotAds > 1000 & dollars_data$NonComp == 1, 1, 0)
#colnames(dollars_data)
# put Nonconp and Treated in the first columns
dollars_data <- dollars_data %>%
select(Treated, TotAds, NonComp, everything())
head(dollars_data)
# How many treated vs not treated
dollars_data %>%
group_by(Treated) %>%
summarise(n = n())
# Estimate the mean contribution for each level of the Treated variable
dollars_data %>%
group_by(Treated) %>%
summarise(mean_contribution = mean(Cont, na.rm = TRUE))
# Estimate Naive ATE
lm_naive <- lm(Cont ~ Treated, data = dollars_data)
naive_ate <- coef(lm_naive)[2]
cat("Naive ATE: ", naive_ate, "\n")
# Select desired columns and remove missing values before logistic regression
selected_data <- dollars_data %>%
select(Treated, Cont, TotalPop, MedianHHInc, PercentOver65, PercentWhite, PercentBlack,
PercentHispanic, Rural, Urban, repvote00, repvote04, RepubVote, RepubShare,
per_hsgrads, per_collegegrads, density) %>%
na.omit()
# Fit a logistic regression for the propensity score
ps_model <- glm(Treated ~ TotalPop + MedianHHInc + PercentOver65 + PercentWhite +
PercentBlack + PercentHispanic + Rural + Urban + repvote00 + repvote04 +
RepubVote + RepubShare + per_hsgrads + per_collegegrads + density,
data = selected_data, family = binomial(link = "logit"))
# Add the propensity score to the dataset
selected_data$pscore <- predict(ps_model, type = "response")
# Reorganize the columns so pscor is the first column
selected_data <- selected_data %>%
select(pscore, everything())
selected_data %>%
head()
# Plot the distribution of the propensity score for the treated and non-treated observations
selected_data %>%
ggplot(aes(x = pscore, fill = factor(Treated))) +
geom_density(alpha = 0.5) +
labs(title = "Density plot of the propensity score split by treatment status",
x = "Propensity Score",
y = "Density")
# Perform propensity score matching
library(MatchIt)
# Create a MatchIt object
match_model <- matchit(Treated ~ pscore, data = selected_data, method = "nearest", replace = TRUE)
# Summary of the matching
summary(match_model)
# Match the data
matched_data <- match.data(match_model)
# Check balance in pre-treatment covariates before matching
tableone_before <- CreateTableOne(vars = c("TotalPop", "MedianHHInc", "PercentOver65", "PercentWhite",
"PercentBlack", "PercentHispanic", "Rural", "Urban",
"repvote00", "repvote04", "RepubVote", "RepubShare",
"per_hsgrads", "per_collegegrads", "density"),
strata = "Treated", data = selected_data)
# Check balance in pre-treatment covariates after matching
tableone_after <- CreateTableOne(vars = c("TotalPop", "MedianHHInc", "PercentOver65", "PercentWhite",
"PercentBlack", "PercentHispanic", "Rural", "Urban",
"repvote00", "repvote04", "RepubVote", "RepubShare",
"per_hsgrads", "per_collegegrads", "density"),
strata = "Treated", data = matched_data)
# Check balance in pre-treatment covariates before matching
print(tableone_before <- CreateTableOne(vars = c("TotalPop", "MedianHHInc", "PercentOver65", "PercentWhite",
"PercentBlack", "PercentHispanic", "Rural", "Urban",
"repvote00", "repvote04", "RepubVote", "RepubShare",
"per_hsgrads", "per_collegegrads", "density"),
strata = "Treated", data = selected_data))
# Check balance in pre-treatment covariates after matching
print(tableone_after <- CreateTableOne(vars = c("TotalPop", "MedianHHInc", "PercentOver65", "PercentWhite",
"PercentBlack", "PercentHispanic", "Rural", "Urban",
"repvote00", "repvote04", "RepubVote", "RepubShare",
"per_hsgrads", "per_collegegrads", "density"),
strata = "Treated", data = matched_data))
# Subset the treated group and their matched controls
treated_units <- matched_data %>% filter(Treated == 1)
control_units <- matched_data %>% filter(Treated == 0)
# Calculate the mean outcome for each group
mean_treated <- mean(treated_units$Cont, na.rm = TRUE)
mean_control <- mean(control_units$Cont, na.rm = TRUE)
# Estimate ATT as the difference-in-means
att_estimate <- mean_treated - mean_control
# print
cat("Estimated ATT (Difference-in-Means):", round(att_estimate, 2), "\n")
